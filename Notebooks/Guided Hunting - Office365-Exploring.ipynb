{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Title: Office 365 Explorer\n**Notebook Version:** 1.0<br>\n**Python Version:** Python 3.6 (including Python 3.6 - AzureML)<br>\n**Required Packages**: kqlmagic, msticpy, pandas, numpy, matplotlib, seaborn, networkx, ipywidgets, ipython, scikit_learn, folium, maxminddb_geolite2, holoviews<br>\n**Platforms Supported**:\n- Azure Notebooks Free Compute\n- Azure Notebooks DSVM\n- OS Independent\n\n**Data Sources Required**:\n- Log Analytics - OfficeActivity, IPLocation, Azure Network Analytics\n\n## Description:\nBrings together a series of queries and visualizations to help you investigate the security status of Office 365 subscription and individual user activities.\n- The first section focuses on Tenant-Wide data queries and analysis\n- The second section allows you to focus on individial accounts and examine them for any suspicious activity.\n\nThis notebook is intended to be illustrative of the types of data available in Office 365 Activity data and how to query and use them. It is not meant to be used as a prescriptive guide to how to navigate through the data. Feel free to experiment and submit anything interesting you find to the community.\n\n## <font color=\"red\">Warning this notebook is suitable for medium and small Office 365 Subscriptions</font>\nA version tuned for large enterprise subscriptions (10,000+) users is under development"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id=\"contents\"></a>\n# Table of Contents\n- [Setup and Authenticate](#setup)\n- [Office 365 Activity](#o365)\n  - [Tenant-wide Information](#tenant_info)\n    - [AAD Operations - Account Modifications](#aad_ops)\n    - [Logon Anomalies](#logon_anomalies)\n    - [Activity Summary](#activity_summary)\n    - [Variability of IP Address for users](#ip_variability)\n    - [Accounts with multiple IPs and Geolocations](#acct_multi_geo)\n    - [User Logons with > N IP Address](#acct_multi_ips)\n    - [Operation Types by Location and IP](#ip_op_matrix)\n    - [Geolocation Map of Client IPs](#geo_map_tenant)\n    - [Distinct User Agent Strings in Use](#distinct_uas)\n    - [Graphical Activity Timeline](#op_timeline)\n    - [Users With largest Activity Type Count](#user_activity_counts)\n  - [Office User Investigation](#o365_user_inv)\n    - [Activity Summary](#user_act_summary)\n    - [Operation Breakdown for User](#user_op_count)\n    - [IP Count for Different User Operations](#user_ip_counts)\n    - [Activity Timeline](#user_act_timeline)\n    - [User IP GeoMap](#user_geomap)\n    - [Check for User IPs in Azure Network Flow Data](#ips_in_azure)\n  - [Rare Combinations of Country/UserAgent/Operation Type](#o365_cluster)\n- [Appendices](#appendices)\n  - [Saving data to Excel](#appendices)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='setup'></a>[Contents](#contents)\n# Setup\n\nMake sure that you have installed packages specified in the setup (uncomment the lines to execute)\n\n## Install Packages\nThe first time this cell runs for a new Azure Notebooks project or local Python environment it will take several minutes to download and install the packages. In subsequent runs it should run quickly and confirm that package dependencies are already installed. Unless you want to upgrade the packages you can feel free to skip execution of the next cell.\n\nIf you see any import failures (```ImportError```) in the notebook, please re-run this cell and answer 'y', then re-run the cell where the failure occurred.\n\nNote you may see some warnings about package incompatibility with certain packages. This does not affect the functionality of this notebook but you may need to upgrade the packages producing the warnings to a more recent version."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\nMIN_REQ_PYTHON = (3,6)\nif sys.version_info < MIN_REQ_PYTHON:\n    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n    print('or later is selected as the active kernel.')\n    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)\n\n# Package Installs - try to avoid if they are already installed\ntry:\n    import msticpy.sectools as sectools\n    import Kqlmagic\n    from dns import reversename, resolver\n    from ipwhois import IPWhois\n    import folium\n    \n    print('If you answer \"n\" this cell will exit with an error in order to avoid the pip install calls,')\n    print('This error can safely be ignored.')\n    resp = input('msticpy and Kqlmagic packages are already loaded. Do you want to re-install? (y/n)')\n    if resp.strip().lower() != 'y':\n        sys.exit('pip install aborted - you may skip this error and continue.')\n    else:\n        print('After installation has completed, restart the current kernel and run '\n              'the notebook again skipping this cell.')\nexcept ImportError:\n    pass\n\nprint('\\nPlease wait. Installing required packages. This may take a few minutes...')\n!pip install git+https://github.com/microsoft/msticpy --upgrade --user\n!pip install Kqlmagic --no-cache-dir --upgrade --user\n!pip install seaborn --upgrade --user\n!pip install holoviews --upgrade --user\n!pip install dnspython --upgrade --user \n!pip install ipwhois --upgrade --user \n!pip install folium --upgrade --user\n\n# Uncomment to refresh the maxminddb database\n# !pip install maxminddb-geolite2 --upgrade \n\nprint('To ensure that the latest versions of the installed libraries '\n      'are used, please restart the current kernel and run '\n      'the notebook again skipping this cell.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Imports\nimport sys\nimport warnings\n\nMIN_REQ_PYTHON = (3,6)\nif sys.version_info < MIN_REQ_PYTHON:\n    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n    print('or later is selected as the active kernel.')\n    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)\n\nimport numpy as np\nfrom IPython import get_ipython\nfrom IPython.display import display, HTML, Markdown\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport networkx as nx\n\nimport pandas as pd\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_colwidth', 100)\n\nimport msticpy.sectools as sectools\nimport msticpy.nbtools as mas\nimport msticpy.nbtools.kql as qry\nimport msticpy.nbtools.nbdisplay as nbdisp\n\n# Some of our dependencies (networkx) still use deprecated Matplotlib\n# APIs - we can't do anything about it so suppress them from view\nfrom matplotlib import MatplotlibDeprecationWarning\nwarnings.simplefilter(\"ignore\", category=MatplotlibDeprecationWarning)\n\nWIDGET_DEFAULTS = {'layout': widgets.Layout(width='95%'),\n                   'style': {'description_width': 'initial'}}\ndisplay(HTML(mas.util._TOGGLE_CODE_PREPARE_STR))\nHTML('''\n    <script type=\"text/javascript\">\n        IPython.notebook.kernel.execute(\"nb_query_string='\".concat(window.location.search).concat(\"'\"));\n    </script>\n    ''');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [
          "remove"
        ]
      },
      "cell_type": "markdown",
      "source": "### Get WorkspaceId\nTo find your Workspace Id go to [Log Analytics](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.OperationalInsights%2Fworkspaces). Look at the workspace properties to find the ID."
    },
    {
      "metadata": {
        "tags": [
          "todo"
        ],
        "trusted": false
      },
      "cell_type": "code",
      "source": "import os\nfrom msticpy.nbtools.wsconfig import WorkspaceConfig\nws_config_file = 'config.json'\n\nWORKSPACE_ID = None\nTENANT_ID = None\ntry:\n    ws_config = WorkspaceConfig(ws_config_file)\n    display(Markdown(f'Read Workspace configuration from local config.json for workspace **{ws_config[\"workspace_name\"]}**'))\n    for cf_item in ['tenant_id', 'subscription_id', 'resource_group', 'workspace_id', 'workspace_name']:\n        display(Markdown(f'**{cf_item.upper()}**: {ws_config[cf_item]}'))\n                     \n    if ('cookiecutter' not in ws_config['workspace_id'] or\n            'cookiecutter' not in ws_config['tenant_id']):\n        WORKSPACE_ID = ws_config['workspace_id']\n        TENANT_ID = ws_config['tenant_id']\nexcept:\n    pass\n\nif not WORKSPACE_ID or not TENANT_ID:\n    display(Markdown('**Workspace configuration not found.**\\n\\n'\n                     'Please go to your Log Analytics workspace, copy the workspace ID'\n                     ' and/or tenant Id and paste here.<br> '\n                     'Or read the workspace_id from the config.json in your Azure Notebooks project.'))\n    ws_config = None\n    ws_id = mas.GetEnvironmentKey(env_var='WORKSPACE_ID',\n                              prompt='Please enter your Log Analytics Workspace Id:', auto_display=True)\n    ten_id = mas.GetEnvironmentKey(env_var='TENANT_ID',\n                              prompt='Please enter your Log Analytics Tenant Id:', auto_display=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Authenticate to Log Analytics\nIf you are using user/device authentication, run the following cell. \n- Click the 'Copy code to clipboard and authenticate' button.\n- This will pop up an Azure Active Directory authentication dialog (in a new tab or browser window). The device code will have been copied to the clipboard. \n- Select the text box and paste (Ctrl-V/Cmd-V) the copied value. \n- You should then be redirected to a user authentication page where you should authenticate with a user account that has permission to query your Log Analytics workspace.\n\nUse the following syntax if you are authenticating using an Azure Active Directory AppId and Secret:\n```\n%kql loganalytics://tenant(aad_tenant).workspace(WORKSPACE_ID).clientid(client_id).clientsecret(client_secret)\n```\ninstead of\n```\n%kql loganalytics://code().workspace(WORKSPACE_ID)\n```\n\nNote: you may occasionally see a JavaScript error displayed at the end of the authentication - you can safely ignore this.<br>\nOn successful authentication you should see a ```popup schema``` button."
    },
    {
      "metadata": {
        "tags": [
          "todo"
        ],
        "trusted": false
      },
      "cell_type": "code",
      "source": "if not WORKSPACE_ID or not TENANT_ID:\n    try:\n        WORKSPACE_ID = ws_id.value\n        TENANT_ID = ten_id.value\n    except NameError:\n        raise ValueError('No workspace or Tenant Id.')\n\nmas.kql.load_kql_magic()\n%kql loganalytics://code().tenant(TENANT_ID).workspace(WORKSPACE_ID)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%kql search * | summarize RowCount=count() by Type | project-rename Table=Type\nla_table_set = _kql_raw_result_.to_dataframe()\ntable_index = la_table_set.set_index('Table')['RowCount'].to_dict()\ndisplay(Markdown('Current data in workspace'))\ndisplay(la_table_set.T)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='o365'></a>[Contents](#contents)\n# Office 365 Activity"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Log Analytics Queries"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "if ('OfficeActivity' not in table_index or\n        table_index['OfficeActivity'] == 0):\n    display(Markdown('<font color=\"red\"><h2>Warning. Office Data not available.</h2></font><br>'\n                     'Either Office 365 data has not been imported into the workspace or'\n                     ' the OfficeActivity table is empty.<br>'\n                     'This workbook is not useable with the current workspace.'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from msticpy.sectools.geoip import GeoLiteLookup\niplocation = GeoLiteLookup()\n\n# Queries\nad_changes_query = '''\nOfficeActivity\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where RecordType == 'AzureActiveDirectory'\n| where Operation in ('Add service principal.',\n                      'Change user password.', \n                      'Add user.', \n                      'Add member to role.')\n| where UserType == 'Regular' \n| project OfficeId, TimeGenerated, Operation, OrganizationId, \n          OfficeWorkload, ResultStatus, OfficeObjectId, \n          UserId = tolower(UserId), ClientIP, ExtendedProperties\n'''\n\n\noffice_ops_query = '''\nOfficeActivity\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n| extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n| union (\n    OfficeActivity \n    | where TimeGenerated >= datetime({start})\n    | where TimeGenerated <= datetime({end})\n    | where RecordType !in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n)\n| where UserType == 'Regular'\n'''\n\n\noffice_ops_summary_query = '''\nlet timeRange=ago(30d);\nlet officeAuthentications = OfficeActivity\n| where TimeGenerated >= timeRange\n| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n| extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n| where Operation == \"UserLoggedIn\";\nofficeAuthentications\n| union (\n    OfficeActivity \n    | where TimeGenerated >= timeRange\n    | where RecordType !in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n)\n| where UserType == 'Regular'\n| extend RecordOp = strcat(RecordType, '-', Operation)\n| summarize OperationCount=count() by RecordType, Operation, UserId, UserAgent, ClientIP, bin(TimeGenerated, 1h)\n// render timeline\n'''\n\n\noffice_logons_byua_query = '''\nlet end = datetime({end});\nlet threshold={threshold};\nlet start = end - 1d;\nlet hist_start = start - 30d;\nlet hist_end = end;\nlet officeAuthentications = OfficeActivity\n| where TimeGenerated >= hist_start\n| where TimeGenerated <= hist_end\n| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n| extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n| where Operation == \"UserLoggedIn\";\nlet lookupWindow = end - start;\nlet lookupBin = lookupWindow / 2.0; \nofficeAuthentications \n| project-rename Start = TimeGenerated\n| extend TimeKey = bin(Start, lookupBin)\n| join kind = inner (\n    officeAuthentications\n    | project-rename End = TimeGenerated\n    | extend TimeKey = range(bin(End - lookupWindow, lookupBin), bin(End, lookupBin), lookupBin)\n    | mvexpand TimeKey to typeof(datetime)\n) on UserAgent, TimeKey\n| project timeSpan = End - Start, UserId, ClientIP , UserAgent , Start, End\n| summarize Count_ClientIP = dcount(ClientIP) by UserId\n| where Count_ClientIP > threshold\n| join kind=inner (  \n    officeAuthentications\n    | summarize minTime=min(TimeGenerated), maxTime=max(TimeGenerated) by UserId, UserAgent, ClientIP\n) on UserAgent\n'''\n\noffice_logons_byuser_query = '''\nlet end = datetime({end});\nlet start = datetime({start});\nlet threshold={threshold};\nlet officeAuthentications = OfficeActivity\n| where TimeGenerated >= start\n| where TimeGenerated <= end\n| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n| extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n| where Operation == \"UserLoggedIn\";\nlet lookupWindow = 1d;\nlet lookupBin = lookupWindow / 2.0; \nofficeAuthentications \n| project-rename Start = TimeGenerated\n| extend TimeKey = bin(Start, lookupBin)\n| join kind = inner (\n    officeAuthentications\n    | project-rename End = TimeGenerated\n    | extend TimeKey = range(bin(End - lookupWindow, lookupBin), bin(End, lookupBin), lookupBin)\n    | mvexpand TimeKey to typeof(datetime)\n) on UserId, TimeKey\n| project timeSpan = End - Start, UserId, ClientIP , UserAgent, Start, End\n| summarize Count_ClientIP = dcount(ClientIP) by UserId\n| where Count_ClientIP > threshold\n| join kind=inner (  \n    officeAuthentications\n    | summarize minTime=min(TimeGenerated), maxTime=max(TimeGenerated) by UserId, UserAgent, ClientIP\n) on UserId\n'''\n\n# %kql -query office_logons_query\n# office_logons_df = _kql_raw_result_.to_dataframe()\n\n#\n# Description: New user agents associated with a clientIP for sharepoint file uploads/downloads. \n#\n# DataSource: #OfficeActivity\n#\n# Techniques: #Exfiltration\n#\nnew_user_agents = '''\nlet end = datetime({end});\nlet start = datetime({end});\nlet hist_start = start - 30d;\nlet hist_end = start;\nlet historicalUA =\nOfficeActivity\n| where TimeGenerated >= hist_start\n| where TimeGenerated <= hist_end\n| where UserType == 'Regular'\n| summarize op_count = count() by UserId, UserAgent, RecordType, Operation;\nlet recentUA = OfficeActivity\n| where TimeGenerated >= start\n| where TimeGenerated <= end\n| where UserType == 'Regular'\n| summarize op_count = count() by UserId, UserAgent, RecordType, Operation;\nrecentUA | join kind=leftanti (\n   historicalUA \n) on UserId, UserAgent\n| where not(isempty(UserId))\n'''\n\nuser_logon_anom_query = '''\nlet LogonEvents=() {{\nlet logonFail=OfficeActivity\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\") and ResultStatus =~ \"Failed\"\n| project  TimeGenerated, AccountName=split(UserId, \"@\").[0], AccountDomain = iff(RecordType == \"AzureActiveDirectoryAccountLogon\",UserDomain,split(UserId, \"@\").[1]), UserId, IpAddress=ClientIP, OrganizationId, \nActionType=\"LogonFailure\";\nlet logonSuccess=OfficeActivity\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\") and ResultStatus =~ \"Succeeded\"\n| project  TimeGenerated, AccountName=split(UserId, \"@\").[0], AccountDomain = iff(RecordType == \"AzureActiveDirectoryAccountLogon\",UserDomain,split(UserId, \"@\").[1]), UserId, IpAddress=ClientIP, OrganizationId, \nActionType=\"Logon\";\n logonFail | union logonSuccess}}; \nlet logonSummary =\n LogonEvents \n| summarize count() by ActionType, IpAddress, tostring(AccountName), tostring(AccountDomain), UserId, OrganizationId, bin(TimeGenerated, 1m); \nlet logon_success = logonSummary | where ActionType == \"Logon\";\nlet logon_fail = logonSummary | where ActionType == \"LogonFailure\";\nlogon_fail | join kind = leftouter (logon_success) on  IpAddress\n| project TimeGenerated, IpAddress, failCount=count_, AccountName, OrganizationId, UserId, successCount=count_1 \n| extend successRate = 1.0*successCount/(successCount+failCount)\n| project TimeGenerated, IpAddress, AccountName, successRate, failCount, successCount, UserId, OrganizationId\n'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# set the origin time to the time of our alert\no365_query_times = mas.QueryTime(units='hours',\n                           before=24, after=1, max_before=60, max_after=20)\no365_query_times.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='tenant_info'></a>[Contents](#contents)\n## Tenant-wide Information"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='activity_summary'></a>[Contents](#contents)\n### Summary of O365 Activity Types\n#### <font color=\"red\">Warning this query can be time consuming for large O365 subscriptions</font>"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('Getting data...', end=' ')\no365_query = office_ops_summary_query.format(start = o365_query_times.start, \n                                             end=o365_query_times.end)\n%kql -query o365_query\noffice_ops_summary_df = _kql_raw_result_.to_dataframe()\nprint('done.')\n(office_ops_summary_df\n .assign(UserId = lambda x: x.UserId.str.lower())\n .groupby(['RecordType', 'Operation'])\n .aggregate({'ClientIP': 'nunique',\n             'UserId': 'nunique',\n             'OperationCount': 'sum'}))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='ip_variability'></a>[Contents](#contents)\n### Variability of IP Address for users"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "unique_ip_op_ua = (office_ops_summary_df.assign(UserId = lambda x: x.UserId.str.lower())\n                   .groupby(['UserId', 'Operation'])\n                   .aggregate({'ClientIP': 'nunique', 'OperationCount': 'sum'})).reset_index()\n\nuser_ip_op = sns.catplot(x=\"ClientIP\", y=\"UserId\", hue='Operation', data=unique_ip_op_ua, height=5, aspect=2)\nuser_ip_op.fig.suptitle('Variability of IP Address Usage by user');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='aad_ops'></a>\n### AAD Operations Changes to users and groups\n\n#### <font color=\"red\">WARNING: due to recent changes in data format this query is not currently functional</font>"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('Getting data...', end=' ')\no365_query = ad_changes_query.format(start = o365_query_times.start, \n                                     end=o365_query_times.end)\n%kql -query o365_query\nad_changes_df = _kql_raw_result_.to_dataframe()\nprint('done.')\nad_changes_df[['TimeGenerated', 'Operation', \n       'OfficeWorkload', 'ResultStatus', 'OfficeObjectId', 'UserId',\n       'ClientIP']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='logon_anomalies'></a>[Contents](#contents)\n### Logon Anomalies\nLogon failures from an ipaddress that then succeed.\n\n#### <font color=\"red\">WARNING: due to recent changes in data format this query is not currently functional</font>"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('Getting data...', end=' ')\no365_query = user_logon_anom_query.format(start = o365_query_times.start, \n                                          end=o365_query_times.end)\n%kql -query o365_query\nuser_logon_anom_df = _kql_raw_result_.to_dataframe()\nprint('done.')\nuser_logon_anom_df.sort_values('failCount')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "office_ops_summary_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='acct_multi_geo'></a>[Contents](#contents)\n### Accounts with multiple IPs and Geolocations"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "restrict_cols = ['RecordType', 'TimeGenerated', 'Operation',\n                 'UserId', 'ClientIP', 'UserAgent']\noffice_ops_summary = office_ops_summary_df[restrict_cols].assign(UserId = lambda x: x.UserId.str.lower())\nunique_ip_op_ua['ClientIPCount'] = unique_ip_op_ua['ClientIP']\noffice_ops_merged = pd.merge(unique_ip_op_ua.query('ClientIP > 1').drop(columns='ClientIP'), \n                             office_ops_summary,\n                             on=['UserId', 'Operation'])\n\nclient_ips = office_ops_merged.query('ClientIP != \"<null>\" & ClientIP != \"\"')['ClientIP'].drop_duplicates().tolist()\nip_entities = []\nfor ip in client_ips:\n    ip_entity = mas.IpAddress(Address=ip)\n    iplocation.lookup_ip(ip_entity=ip_entity)\n    ip_dict = {'Address': ip_entity.Address}\n    ip_dict.update(ip_entity.Location.properties)\n    ip_entities.append(pd.Series(ip_dict))\n\nip_locs_df = pd.DataFrame(data=ip_entities)\nip_locs_df\n\noffice_ops_summary_ip_loc = pd.merge(office_ops_merged, \n                                     ip_locs_df, left_on='ClientIP', \n                                     right_on='Address', how='left')\n\n(office_ops_summary_ip_loc.groupby(['UserId', 'CountryCode', 'City'])\n                   .aggregate({'ClientIP': 'nunique', 'OperationCount': 'sum'})).reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='acct_multi_ips'></a>[Contents](#contents)\n### User Logons where User has logged on from > N IP Address in period"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "th_wgt = widgets.IntSlider(value=1, min=1, max=50, step=1, description='Set IP Count Threshold', **WIDGET_DEFAULTS)\nth_wgt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('Getting data...', end=' ')\no365_query = office_logons_byuser_query.format(start = o365_query_times.start, \n                                               end=o365_query_times.end,\n                                               threshold=th_wgt.value)\n%kql -query o365_query\noffice_logons_byuser_df = _kql_raw_result_.to_dataframe()\nprint('done.')\noffice_logons_byuser_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='ip_op_matrix'></a>[Contents](#contents)\n### Matrix of Selected Operation Types by Location and IP"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('Getting data...', end=' ')\no365_query = office_ops_query.format(start=o365_query_times.start, \n                                     end=o365_query_times.end)\n%kql -query o365_query\noffice_ops_df = _kql_raw_result_.to_dataframe()\nprint('done.') \n\n# Get Locations for distinct IPs\nclient_ips = office_ops_df.query('ClientIP != \"<null>\" & ClientIP != \"\"')['ClientIP'].drop_duplicates().tolist()\nip_entities = []\nfor ip in client_ips:\n    ip_entity = mas.IpAddress(Address=ip)\n    iplocation.lookup_ip(ip_entity=ip_entity)\n    ip_dict = {'Address': ip_entity.Address}\n    ip_dict.update(ip_entity.Location.properties)\n    ip_entities.append(pd.Series(ip_dict))\n\nip_locs_df = pd.DataFrame(data=ip_entities)\n\n# Get rid of unneeded columns\nrestrict_cols = ['OfficeId', 'RecordType', 'TimeGenerated', 'Operation',\n                 'OrganizationId', 'UserType', 'UserKey', 'OfficeWorkload',\n                 'ResultStatus', 'OfficeObjectId', 'UserId', 'ClientIP','UserAgent']\noffice_ops_restr = office_ops_df[restrict_cols]\n\n# Merge main DF with IP location data\noffice_ops_locs = pd.merge(office_ops_restr, ip_locs_df, how='right', left_on='ClientIP', right_on='Address',\n         indicator=True)\n\nlimit_op_types = ['FileDownloaded', 'FileModified','FileUploaded',\n                  'UserLoggedIn','UserLoginFailed','Add member to role.',\n                 'Add user.','Change user password.', 'Update user.']\n\noffice_ops_locs = office_ops_locs[office_ops_locs.Operation.isin(limit_op_types)]\n\n# Calculate operations grouped by location and operation type\ncm = sns.light_palette(\"yellow\", as_cmap=True)\ncountry_by_op_count = (office_ops_locs[['Operation', 'RecordType', 'CountryCode', 'City']]\n                        .groupby(['CountryCode', 'City', 'Operation'])\n                        .count())\ndisplay(country_by_op_count.unstack().fillna(0).rename(columns={'RecordType':'OperationCount'})\n        .style.background_gradient(cmap=cm))\n\n# Group by Client IP, Country, Operation\nclientip_by_op_count = (office_ops_locs[['ClientIP', 'Operation', 'RecordType', 'CountryCode']]\n                        .groupby(['ClientIP', 'CountryCode', 'Operation'])\n                        .count())\n\n(clientip_by_op_count.unstack().fillna(0).rename(columns={'RecordType':'OperationCount'})\n .style.background_gradient(cmap=cm))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='geo_map_tenant'></a>[Contents](#contents)\n### Geolocation Map of Client IPs"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from msticpy.nbtools.foliummap import FoliumMap\nfolium_map = FoliumMap()\n\ndef get_row_ip_loc(row):\n    try:\n        _, ip_entity = iplocation.lookup_ip(ip_address=row.ClientIP)\n        return ip_entity\n    except ValueError:\n        return None\n    \noff_ip_locs = (office_ops_df[['ClientIP']]\n                   .drop_duplicates()\n                   .apply(get_row_ip_loc, axis=1)\n                   .tolist())\nip_locs = [ip_list[0] for ip_list in off_ip_locs if ip_list]\n    \ndisplay(HTML('<h3>External IP Addresses seen in Office Activity</h3>'))\ndisplay(HTML('Numbered circles indicate multiple items - click to expand.'))\n\n\nicon_props = {'color': 'purple'}\nfolium_map.add_ip_cluster(ip_entities=ip_locs,\n                          **icon_props)\ndisplay(folium_map.folium_map)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='distinct_uas'></a>[Contents](#contents)\n### Distinct User Agent Strings in Use\n\n#### <font color=\"red\">WARNING: due to recent changes in data format this query is not currently functional</font>"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\ndisplay(Markdown('### IPs and User Agents - frequency of use'))\ndisplay(Markdown('Distinct UserAgents by num of operations'))\noffice_ops_df[['UserAgent', 'Operation']].groupby(['UserAgent']).count().rename(columns={'Operation':'OpCount'})\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='op_timeline'></a>[Contents](#contents)\n### Graphical Activity Timeline"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    display(Markdown('### Change in rate of Activity Class (RecordType) and Operation'))\n    sns.relplot(data=office_ops_summary_df, x='TimeGenerated', y='OperationCount', kind='line', aspect=2, \n                hue='RecordType')\n    sns.relplot(data=office_ops_summary_df.query('RecordType == \"SharePointFileOperation\"'), \n                x='TimeGenerated', y='OperationCount', hue='Operation', kind='line', aspect=2)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='user_activity_counts'></a>[Contents](#contents)\n### Users With largest Activity Type Count"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    display(Markdown('### Identify Users/IPs with largest operation count'))\n    office_ops = office_ops_summary_df.assign(Account=lambda x: \n                                              (x.UserId.str.extract('([^@]+)@.*', expand=False)).str.lower())\n\n    limit_op_types = ['FileDownloaded', 'FileModified','FileUploaded',\n                      'UserLoggedIn','UserLoginFailed','Add member to role.',\n                     'Add user.','Change user password.', 'Update user.']\n    office_ops = office_ops[office_ops.Operation.isin(limit_op_types)]\n    \n    sns.catplot(data=office_ops, y='Account', x='OperationCount', \n                hue='Operation', aspect=2)\n    display(office_ops.pivot_table('OperationCount', index=['Account'], \n                                   columns='Operation').style.bar(color='orange', align='mid'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "new_df = office_ops_df[['OfficeId', 'RecordType', 'TimeGenerated', 'Operation',\n       'OrganizationId', 'UserType', 'UserKey', 'OfficeWorkload',\n       'ResultStatus', 'OfficeObjectId', 'UserId', 'ClientIP','UserAgent']]\npd.merge(new_df, ip_locs_df, how='left', left_on='ClientIP', right_on='Address')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='o365_user_inv'></a>[Contents](#contents)\n## Office User Investigation"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# set the origin time to the time of our alert\no365_query_times_user = mas.QueryTime(units='days',\n                           before=10, after=1, max_before=60, max_after=20, auto_display=True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "distinct_users = office_ops_df[['UserId']].sort_values('UserId')['UserId'].str.lower().drop_duplicates().tolist()\ndistinct_users\nuser_select = mas.SelectString(description='Select User Id', item_list=distinct_users, auto_display=True)\n                               # (items=distinct_users)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='user_act_summary'></a>[Contents](#contents)\n### Activity Summary"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Provides a summary view of a given account's activity\n# For use when investigating an account that has been identified as having associated suspect activity or been otherwise compromised. \n# All office activity by UserName using UI to set Time range\n# Tags: #Persistence, #Discovery, #Lateral Movement, #Collection\n\nuser_activity_query = '''\nOfficeActivity\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where UserKey has \"{user}\" or UserId has \"{user}\"\n'''\nprint('Getting data...', end=' ')\no365_query = user_activity_query.format(start=o365_query_times_user.start, \n                                        end=o365_query_times_user.end,\n                                        user=user_select.value)\n%kql -query o365_query\nuser_activity_df = _kql_raw_result_.to_dataframe()\nprint('done.')\nuser_activity_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='user_op_count'></a>[Contents](#contents)\n### Operation Breakdown for User"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_df = (user_activity_df[['OfficeId', 'RecordType', 'TimeGenerated', 'Operation',\n                           'ResultStatus', 'UserId', 'ClientIP','UserAgent']]\n         .groupby(['Operation', 'ResultStatus', 'ClientIP'])\n         .aggregate({'OfficeId': 'count'})\n         .rename(columns={'OfficeId': 'OperationCount', 'ClientIP': 'IPCount'})\n         .reset_index())\nsns.catplot(x='OperationCount', y=\"Operation\", hue=\"ClientIP\", jitter=False, data=my_df, aspect=2.5);\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='user_ip_counts'></a>[Contents](#contents)\n### IP Count for Different User Operations "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_df2 = (user_activity_df[['OfficeId', 'RecordType', 'TimeGenerated', 'Operation',\n                           'ResultStatus', 'UserId', 'ClientIP','UserAgent']]\n         .groupby(['Operation'])\n         .aggregate({'OfficeId': 'count', 'ClientIP': 'nunique'})\n         .rename(columns={'OfficeId': 'OperationCount', 'ClientIP': 'IPCount'})\n         .reset_index())\nsns.barplot(x='IPCount', y=\"Operation\", data=my_df2);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='user_act_timeline'></a>[Contents](#contents)\n### Activity Timeline"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nbdisp.display_timeline(data=user_activity_df,\n                         title='Office Operations',\n                         source_columns=['OfficeWorkload', 'Operation', 'ClientIP', 'ResultStatus'],\n                         height=200)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='user_geomap'></a>[Contents](#contents)\n### User IP GeoMap"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_row_ip_loc(row):\n    try:\n        _, ip_entity = iplocation.lookup_ip(ip_address=row.ClientIP)\n        return ip_entity\n    except ValueError:\n        return None\n    \nfrom msticpy.nbtools.foliummap import FoliumMap\nfolium_map = FoliumMap()\noff_ip_locs = (user_activity_df[['ClientIP']]\n                   .drop_duplicates()\n                   .apply(get_row_ip_loc, axis=1)\n                   .tolist())\nip_locs = [ip_list[0] for ip_list in off_ip_locs if ip_list]\n    \ndisplay(HTML('<h3>External IP Addresses seen in Office Activity</h3>'))\ndisplay(HTML('Numbered circles indicate multiple items - click to expand.'))\n\n\nicon_props = {'color': 'purple'}\nfolium_map.add_ip_cluster(ip_entities=ip_locs,\n                          **icon_props)\ndisplay(folium_map.folium_map)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='ips_in_azure'></a>[Contents](#contents)\n### Check for User IPs in Azure Network Flow Data\nThe full data is available in the Dataframe ```az_net_query_byip```"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "if ('AzureNetworkAnalytics_CL' not in table_index or\n        table_index['AzureNetworkAnalytics_CL'] == 0):\n    display(Markdown('<font color=\"red\"><h2>Warning. Azure network flow data not available.</h2></font><br>'\n                     'This section of the notebook is not useable with the current workspace.'))\n    \n# Azure Network Analytics Base Query\naz_net_analytics_query =r'''\nAzureNetworkAnalytics_CL \n| where SubType_s == 'FlowLog'\n| where FlowStartTime_t >= datetime({start})\n| where FlowEndTime_t <= datetime({end})\n| project TenantId, TimeGenerated, \n    FlowStartTime = FlowStartTime_t, \n    FlowEndTime = FlowEndTime_t, \n    FlowIntervalEndTime = FlowIntervalEndTime_t, \n    FlowType = FlowType_s,\n    ResourceGroup = split(VM_s, '/')[0],\n    VMName = split(VM_s, '/')[1],\n    VMIPAddress = VMIP_s, \n    PublicIPs = extractall(@\"([\\d\\.]+)[|\\d]+\", dynamic([1]), PublicIPs_s),\n    SrcIP = SrcIP_s,\n    DestIP = DestIP_s,\n    ExtIP = iif(FlowDirection_s == 'I', SrcIP_s, DestIP_s),\n    L4Protocol = L4Protocol_s, \n    L7Protocol = L7Protocol_s, \n    DestPort = DestPort_d, \n    FlowDirection = FlowDirection_s,\n    AllowedOutFlows = AllowedOutFlows_d, \n    AllowedInFlows = AllowedInFlows_d,\n    DeniedInFlows = DeniedInFlows_d, \n    DeniedOutFlows = DeniedOutFlows_d,\n    RemoteRegion = AzureRegion_s,\n    VMRegion = Region_s\n| extend AllExtIPs = iif(isempty(PublicIPs), pack_array(ExtIP), \n                         iif(isempty(ExtIP), PublicIPs, array_concat(PublicIPs, pack_array(ExtIP)))\n                         )\n| project-away ExtIP\n| mvexpand AllExtIPs\n{where_clause}\n'''\n\n# Build the query parameters\nall_user_ips = user_activity_df['ClientIP'].drop_duplicates().tolist()\nall_user_ips = [ip for ip in all_user_ips if ip and ip != '<null>']\nip_list = ','.join(['\\'{}\\''.format(i) for i in all_user_ips])\n\naz_ip_where = f'''\n| where (AllExtIPs in ({ip_list}) \n        or SrcIP in ({ip_list}) \n        or DestIP in ({ip_list}) \n        ) and \n    (AllowedOutFlows > 0 or AllowedInFlows > 0)'''\nprint('getting data...')\naz_net_query_byip = az_net_analytics_query.format(where_clause=az_ip_where,\n                                                  start=o365_query_times_user.start,\n                                                  end=o365_query_times_user.end)\n\nnet_default_cols = ['FlowStartTime', 'FlowEndTime', 'VMName', 'VMIPAddress', \n                'PublicIPs', 'SrcIP', 'DestIP', 'L4Protocol', 'L7Protocol',\n                'DestPort', 'FlowDirection', 'AllowedOutFlows', \n                'AllowedInFlows']\n\n%kql -query az_net_query_byip\naz_net_comms_df = _kql_raw_result_.to_dataframe()\naz_net_comms_df[net_default_cols]\n\nimport warnings\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    az_net_comms_df['TotalAllowedFlows'] = az_net_comms_df['AllowedOutFlows'] + az_net_comms_df['AllowedInFlows']\n    sns.catplot(x=\"L7Protocol\", y=\"TotalAllowedFlows\", col=\"FlowDirection\", data=az_net_comms_df)\n    sns.relplot(x=\"FlowStartTime\", y=\"TotalAllowedFlows\", \n                col=\"FlowDirection\", kind=\"line\", \n                hue=\"L7Protocol\", data=az_net_comms_df).set_xticklabels(rotation=50)\n\ncols = ['VMName', 'VMIPAddress', 'PublicIPs', 'SrcIP', 'DestIP', 'L4Protocol',\n        'L7Protocol', 'DestPort', 'FlowDirection', 'AllExtIPs', 'TotalAllowedFlows']\nflow_index = az_net_comms_df[cols].copy()\ndef get_source_ip(row):\n    if row.FlowDirection == 'O':\n        return row.VMIPAddress if row.VMIPAddress else row.SrcIP\n    else:\n        return row.AllExtIPs if row.AllExtIPs else row.DestIP\n    \ndef get_dest_ip(row):\n    if row.FlowDirection == 'O':\n        return row.AllExtIPs if row.AllExtIPs else row.DestIP\n    else:\n        return row.VMIPAddress if row.VMIPAddress else row.SrcIP\n\nflow_index['source'] = flow_index.apply(get_source_ip, axis=1)\nflow_index['target'] = flow_index.apply(get_dest_ip, axis=1)\nflow_index['value'] = flow_index['L7Protocol']\n\ncm = sns.light_palette(\"green\", as_cmap=True)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    display(flow_index[['source', 'target', 'value', 'L7Protocol', \n                        'FlowDirection', 'TotalAllowedFlows']]\n            .groupby(['source', 'target', 'value', 'L7Protocol', 'FlowDirection'])\n            .sum().unstack().style.background_gradient(cmap=cm))\n\nnbdisp.display_timeline(data=az_net_comms_df.query('AllowedOutFlows > 0'),\n                         overlay_data=az_net_comms_df.query('AllowedInFlows > 0'),\n                         title='Network Flows (out=blue, in=green)',\n                         time_column='FlowStartTime',\n                         source_columns=['FlowType', 'AllExtIPs', 'L7Protocol', 'FlowDirection'],\n                         height=300)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='o365_cluster'></a>[Contents](#contents)\n## Rare Combinations of Country/UserAgent/Operation Type\nThe dataframe below lists combinations in the time period that had less than 3 instances. This might help you to spot relatively unusual activity."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\nfrom msticpy.sectools.eventcluster import (dbcluster_events, \n                                           add_process_features, \n                                           char_ord_score,\n                                           token_count,\n                                           delim_count)\n\nrestrict_cols = ['OfficeId', 'RecordType', 'TimeGenerated', 'Operation',\n                 'OrganizationId', 'UserType', 'UserKey', 'OfficeWorkload',\n                 'ResultStatus', 'OfficeObjectId', 'UserId', 'ClientIP','UserAgent']\nfeature_office_ops = office_ops_df[restrict_cols]\nfeature_office_ops = ( pd.merge(feature_office_ops, \n                                ip_locs_df, how='left', \n                                left_on='ClientIP', right_on='Address')\n                      .fillna(''))\n\n# feature_office_ops = office_ops_df.copy()\n\nfeature_office_ops['country_num'] = feature_office_ops.apply(lambda x: char_ord_score(x, 'CountryCode') if x.CountryCode else 0, axis=1)\nfeature_office_ops['ua_tokens'] = feature_office_ops.apply(lambda x: char_ord_score(x, 'UserAgent'), axis=1)\nfeature_office_ops['user_num'] = feature_office_ops.apply(lambda x: char_ord_score(x, 'UserId'), axis=1)\nfeature_office_ops['op_num'] = feature_office_ops.apply(lambda x: char_ord_score(x, 'Operation'), axis=1)\n\n# you might need to play around with the max_cluster_distance parameter.\n# decreasing this gives more clusters.\n(clustered_ops, dbcluster, x_data) = dbcluster_events(data=feature_office_ops,\n                                                      cluster_columns=['country_num',\n                                                                       'op_num',\n                                                                       'ua_tokens'],\n                                                      time_column='TimeGenerated',\n                                                      max_cluster_distance=0.0001)\nprint('Number of input events:', len(feature_office_ops))\nprint('Number of clustered events:', len(clustered_ops))\ndisplay(Markdown('#### Rarest combinations'))\ndisplay(clustered_ops[['TimeGenerated', 'RecordType',\n                        'Operation', 'UserId', 'UserAgent', 'ClusterSize',\n                        'OfficeObjectId', 'CountryName']]\n    .query('ClusterSize <= 2')\n    .sort_values('ClusterSize', ascending=True))\ndisplay(Markdown('#### Most common operations'))\ndisplay((clustered_ops[['RecordType', 'Operation', 'ClusterSize']]\n    .sort_values('ClusterSize', ascending=False)\n    .head(10)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true
      },
      "cell_type": "markdown",
      "source": "<a id='appendices'></a>[Contents](#contents)\n# Appendices"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Available DataFrames"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('List of current DataFrames in Notebook')\nprint('-' * 50)\ncurrent_vars = list(locals().keys())\nfor var_name in current_vars:\n    if isinstance(locals()[var_name], pd.DataFrame) and not var_name.startswith('_'):\n        print(var_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "tags": [
          "todo"
        ]
      },
      "cell_type": "markdown",
      "source": "## Saving Data to Excel\nTo save the contents of a pandas DataFrame to an Excel spreadsheet\nuse the following syntax\n```\nwriter = pd.ExcelWriter('myWorksheet.xlsx')\nmy_data_frame.to_excel(writer,'Sheet1')\nwriter.save()\n```"
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "318.996px",
        "width": "320.994px"
      },
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents2",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "351px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "406.193px",
        "left": "1468.4px",
        "right": "20px",
        "top": "120px",
        "width": "456.572px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}