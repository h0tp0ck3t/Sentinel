{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Title: Windows Host Explorer\n**Notebook Version:** 1.0<br>\n**Python Version:** Python 3.6 (including Python 3.6 - AzureML)<br>\n**Required Packages**: kqlmagic, msticpy, pandas, numpy, matplotlib, networkx, ipywidgets, ipython, scikit_learn, dnspython, ipwhois, folium, maxminddb_geolite2, holoviews<br>\n**Platforms Supported**:\n- Azure Notebooks Free Compute\n- Azure Notebooks DSVM\n- OS Independent\n\n**Data Sources Required**:\n- Log Analytics - SecurityAlert, SecurityEvent (EventIDs 4688 and 4624/25), AzureNetworkAnalytics_CL, Heartbeat\n- (Optional) - VirusTotal (with API key)\n\n## Description:\nBrings together a series of queries and visualizations to help you determine the security state of the Windows host or virtual machine that you are investigating.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='toc'></a>\n# Table of Contents\n- [Setup and Authenticate](#setup)\n\n- [Get Host Name](#get_hostname)\n- [Related Alerts](#related_alerts)\n- [Host Logons](#host_logons)\n  - [Failed Logons](#failed_logons)\n  - [Session Processes](#examine_win_logon_sess)\n- [Check for IOCs in Commandline](#cmdlineiocs)\n  - [VirusTotal lookup](#virustotallookup)\n- [Network Data](#comms_to_other_hosts)\n- [Appendices](#appendices)\n  - [Saving data to Excel](#appendices)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='setup'></a>[Contents](#toc)\n# Setup\n\nMake sure that you have installed packages specified in the setup (uncomment the lines to execute)\n\n## Install Packages\nThe first time this cell runs for a new Azure Notebooks project or local Python environment it will take several minutes to download and install the packages. In subsequent runs it should run quickly and confirm that package dependencies are already installed. Unless you want to upgrade the packages you can feel free to skip execution of the next cell.\n\nIf you see any import failures (```ImportError```) in the notebook, please re-run this cell and answer 'y', then re-run the cell where the failure occurred.\n\nNote you may see some warnings about package incompatibility with certain packages. This does not affect the functionality of this notebook but you may need to upgrade the packages producing the warnings to a more recent version."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\nMIN_REQ_PYTHON = (3,6)\nif sys.version_info < MIN_REQ_PYTHON:\n    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n    print('or later is selected as the active kernel.')\n    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)\n\n# Package Installs - try to avoid if they are already installed\ntry:\n    import msticpy.sectools as sectools\n    import Kqlmagic\n    from dns import reversename, resolver\n    from ipwhois import IPWhois\n    import folium\n    \n    print('If you answer \"n\" this cell will exit with an error in order to avoid the pip install calls,')\n    print('This error can safely be ignored.')\n    resp = input('msticpy and Kqlmagic packages are already loaded. Do you want to re-install? (y/n)')\n    if resp.strip().lower() != 'y':\n        sys.exit('pip install aborted - you may skip this error and continue.')\n    else:\n        print('After installation has completed, restart the current kernel and run '\n              'the notebook again skipping this cell.')\nexcept ImportError:\n    pass\n\nprint('\\nPlease wait. Installing required packages. This may take a few minutes...')\n!pip install git+https://github.com/microsoft/msticpy --upgrade --user\n!pip install Kqlmagic --no-cache-dir --upgrade --user\n!pip install seaborn --upgrade --user\n!pip install holoviews --upgrade --user\n!pip install dnspython --upgrade --user \n!pip install ipwhois --upgrade --user \n!pip install folium --upgrade --user\n\n# Uncomment to refresh the maxminddb database\n# !pip install maxminddb-geolite2 --upgrade \nprint('To ensure that the latest versions of the installed libraries '\n      'are used, please restart the current kernel and run '\n      'the notebook again skipping this cell.')",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Check the Kernel->Change Kernel menu and ensure that Python 3.6\nor later is selected as the active kernel.\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Python 3.6 or later is required.\n",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Python 3.6 or later is required.\n\n"
          ]
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Imports\nimport sys\nimport warnings\n\nMIN_REQ_PYTHON = (3,6)\nif sys.version_info < MIN_REQ_PYTHON:\n    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n    print('or later is selected as the active kernel.')\n    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)\n\nimport numpy as np\nfrom IPython import get_ipython\nfrom IPython.display import display, HTML, Markdown\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport networkx as nx\n\nimport pandas as pd\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_colwidth', 100)\n\nimport msticpy.sectools as sectools\nimport msticpy.nbtools as mas\nimport msticpy.nbtools.kql as qry\nimport msticpy.nbtools.nbdisplay as nbdisp\n\nWIDGET_DEFAULTS = {'layout': widgets.Layout(width='95%'),\n                   'style': {'description_width': 'initial'}}\n\n# Some of our dependencies (networkx) still use deprecated Matplotlib\n# APIs - we can't do anything about it so suppress them from view\nfrom matplotlib import MatplotlibDeprecationWarning\nwarnings.simplefilter(\"ignore\", category=MatplotlibDeprecationWarning)\n\ndisplay(HTML(mas.util._TOGGLE_CODE_PREPARE_STR))\nHTML('''\n    <script type=\"text/javascript\">\n        IPython.notebook.kernel.execute(\"nb_query_string='\".concat(window.location.search).concat(\"'\"));\n    </script>\n    ''');",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Check the Kernel->Change Kernel menu and ensure that Python 3.6\nor later is selected as the active kernel.\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Python 3.6 or later is required.\n",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Python 3.6 or later is required.\n\n"
          ]
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "tags": [
          "remove"
        ]
      },
      "cell_type": "markdown",
      "source": "### Get WorkspaceId\nTo find your Workspace Id go to [Log Analytics](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.OperationalInsights%2Fworkspaces). Look at the workspace properties to find the ID."
    },
    {
      "metadata": {
        "tags": [
          "todo"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom msticpy.nbtools.wsconfig import WorkspaceConfig\nws_config_file = 'config.json'\n\nWORKSPACE_ID = None\nTENANT_ID = None\ntry:\n    ws_config = WorkspaceConfig(ws_config_file)\n    display(Markdown(f'Read Workspace configuration from local config.json for workspace **{ws_config[\"workspace_name\"]}**'))\n    for cf_item in ['tenant_id', 'subscription_id', 'resource_group', 'workspace_id', 'workspace_name']:\n        display(Markdown(f'**{cf_item.upper()}**: {ws_config[cf_item]}'))\n                     \n    if ('cookiecutter' not in ws_config['workspace_id'] or\n            'cookiecutter' not in ws_config['tenant_id']):\n        WORKSPACE_ID = ws_config['workspace_id']\n        TENANT_ID = ws_config['tenant_id']\nexcept:\n    pass\n\nif not WORKSPACE_ID or not TENANT_ID:\n    display(Markdown('**Workspace configuration not found.**\\n\\n'\n                     'Please go to your Log Analytics workspace, copy the workspace ID'\n                     ' and/or tenant Id and paste here.<br> '\n                     'Or read the workspace_id from the config.json in your Azure Notebooks project.'))\n    ws_config = None\n    ws_id = mas.GetEnvironmentKey(env_var='WORKSPACE_ID',\n                              prompt='Please enter your Log Analytics Workspace Id:', auto_display=True)\n    ten_id = mas.GetEnvironmentKey(env_var='TENANT_ID',\n                              prompt='Please enter your Log Analytics Tenant Id:', auto_display=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Authenticate to Log Analytics\nIf you are using user/device authentication, run the following cell. \n- Click the 'Copy code to clipboard and authenticate' button.\n- This will pop up an Azure Active Directory authentication dialog (in a new tab or browser window). The device code will have been copied to the clipboard. \n- Select the text box and paste (Ctrl-V/Cmd-V) the copied value. \n- You should then be redirected to a user authentication page where you should authenticate with a user account that has permission to query your Log Analytics workspace.\n\nUse the following syntax if you are authenticating using an Azure Active Directory AppId and Secret:\n```\n%kql loganalytics://tenant(aad_tenant).workspace(WORKSPACE_ID).clientid(client_id).clientsecret(client_secret)\n```\ninstead of\n```\n%kql loganalytics://code().workspace(WORKSPACE_ID)\n```\n\nNote: you may occasionally see a JavaScript error displayed at the end of the authentication - you can safely ignore this.<br>\nOn successful authentication you should see a ```popup schema``` button."
    },
    {
      "metadata": {
        "tags": [
          "todo"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "if not WORKSPACE_ID or not TENANT_ID:\n    try:\n        WORKSPACE_ID = ws_id.value\n        TENANT_ID = ten_id.value\n    except NameError:\n        raise ValueError('No workspace or Tenant Id.')\n\nmas.kql.load_kql_magic()\n%kql loganalytics://code().tenant(TENANT_ID).workspace(WORKSPACE_ID)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%kql search * | summarize RowCount=count() by Type | project-rename Table=Type\nla_table_set = _kql_raw_result_.to_dataframe()\ntable_index = la_table_set.set_index('Table')['RowCount'].to_dict()\ndisplay(Markdown('Current data in workspace'))\ndisplay(la_table_set.T)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='get_hostname'></a>[Contents](#toc)\n# Enter the host name and query time window"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "host_text = widgets.Text(description='Enter the Host name to search for:', **WIDGET_DEFAULTS)\ndisplay(host_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "query_times = mas.QueryTime(units='day', max_before=20, before=5, max_after=1)\nquery_times.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from msticpy.nbtools.entityschema import GeoLocation\nfrom msticpy.sectools.geoip import GeoLiteLookup\niplocation = GeoLiteLookup()\n\n# Get single event - try process creation\nif 'SecurityEvent' not in table_index:\n    raise ValueError('No Windows event log data available in the workspace')\nstart = f'\\'{query_times.start}\\''\nhostname = host_text.value\nfind_host_event_query = r'''\nSecurityEvent\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where Computer has '{hostname}'\n| top 1 by TimeGenerated desc nulls last\n'''.format(start=query_times.start,\n           end=query_times.end,\n           hostname=hostname)\n\nprint('Checking for event data...')\n# Get heartbeat event if available\n%kql -query find_host_event_query\nif _kql_raw_result_.completion_query_info['StatusCode'] == 0:\n    host_event_df = _kql_raw_result_.to_dataframe()\n\nhost_event = None\nhost_entity = None\nif host_event_df.shape[0] > 0:\n    host_entity = mas.Host(src_event=host_event_df.iloc[0])\nif not host_entity:\n    raise LookupError(f'Could not find Windows events the name {hostname}')\n                                                                   \n# Try to get an OMS Heartbeat for this computer\nif 'Heartbeat' in table_index:\n    \n    heartbeat_query = '''\nHeartbeat \n| where Computer == \\'{computer}\\' \n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| top 1 by TimeGenerated desc nulls last\n'''.format(start=query_times.start,\n           end=query_times.end,\n           computer=host_entity.computer)\n\n    print('Getting heartbeat data...')\n    %kql -query heartbeat_query\n\n    if _kql_raw_result_.completion_query_info['StatusCode'] == 0:\n        host_hb = _kql_raw_result_.to_dataframe().iloc[0]\n        host_entity.SourceComputerId = host_hb['SourceComputerId']\n        host_entity.OSType = host_hb['OSType']\n        host_entity.OSMajorVersion = host_hb['OSMajorVersion']\n        host_entity.OSMinorVersion = host_hb['OSMinorVersion']\n        host_entity.ComputerEnvironment = host_hb['ComputerEnvironment']\n        host_entity.OmsSolutions = [sol.strip() for sol in host_hb['Solutions'].split(',')]\n        host_entity.VMUUID = host_hb['VMUUID']\n\n        ip_entity = mas.IpAddress()\n        ip_entity.Address = host_hb['ComputerIP']                                                                \n        geoloc_entity = GeoLocation()\n        geoloc_entity.CountryName = host_hb['RemoteIPCountry']                                                               \n        geoloc_entity.Longitude = host_hb['RemoteIPLongitude']\n        geoloc_entity.Latitude = host_hb['RemoteIPLatitude']\n        ip_entity.Location = geoloc_entity\n        host_entity.IPAddress = ip_entity # TODO change to graph edge    \n\nif 'AzureNetworkAnalytics_CL' in table_index:\n    print('Looking for IP addresses in network flows...')\n    aznet_query = '''\nAzureNetworkAnalytics_CL\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| where VirtualMachine_s has \\'{host}\\'\n| where ResourceType == 'NetworkInterface'\n| top 1 by TimeGenerated desc\n| project PrivateIPAddresses = PrivateIPAddresses_s, \n    PublicIPAddresses = PublicIPAddresses_s\n'''.format(start=query_times.start,\n           end=query_times.end,\n           host=host_entity.HostName)\n    %kql -query aznet_query\n    az_net_df = _kql_raw_result_.to_dataframe()\n\n    def convert_to_ip_entities(ip_str):\n        ip_entities = []\n        if ip_str:\n            if ',' in ip_str:\n                addrs = ip_str.split(',')\n            elif ' ' in ip_str:\n                addrs = ip_str.split(' ')\n            else:\n                addrs = [ip_str]\n            for addr in addrs:\n                ip_entity = mas.IpAddress()\n                ip_entity.Address = addr.strip()\n                iplocation.lookup_ip(ip_entity=ip_entity)\n                ip_entities.append(ip_entity)\n        return ip_entities\n\n    # Add this information to our inv_host_entity\n    retrieved_address=[]\n    if len(az_net_df) == 1:\n        priv_addr_str = az_net_df['PrivateIPAddresses'].loc[0]\n        host_entity.properties['private_ips'] = convert_to_ip_entities(priv_addr_str)\n\n        pub_addr_str = az_net_df['PublicIPAddresses'].loc[0]\n        host_entity.properties['public_ips'] = convert_to_ip_entities(pub_addr_str)\n        retrieved_address = [ip.Address for ip in host_entity.properties['public_ips']]\n    else:\n        if 'private_ips' not in host_entity.properties:\n            host_entity.properties['private_ips'] = []\n        if 'public_ips' not in host_entity.properties:\n            host_entity.properties['public_ips'] = []\n        \nprint(host_entity)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [
          "todo"
        ]
      },
      "cell_type": "markdown",
      "source": "<a id='related_alerts'></a>[Contents](#toc)\n# Related Alerts"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# set the origin time to the time of our alert\nquery_times = mas.QueryTime(units='day',  \n                            max_before=28, max_after=1, before=5)\nquery_times.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "related_alerts_query = r'''\nSecurityAlert\n| where TimeGenerated >= datetime({start})\n| where TimeGenerated <= datetime({end})\n| extend StartTimeUtc = TimeGenerated\n| extend AlertDisplayName = DisplayName\n| extend Computer = '{host}'\n| extend simple_hostname = tostring(split(Computer, '.')[0])\n| where Entities has Computer or Entities has simple_hostname\n     or ExtendedProperties has Computer\n     or ExtendedProperties has simple_hostname\n'''.format(start=query_times.start,\n           end=query_times.end,\n           host=host_entity.HostName)\n%kql -query related_alerts_query\nrelated_alerts = _kql_raw_result_.to_dataframe()\n\nif related_alerts is not None and not related_alerts.empty:\n    host_alert_items = (related_alerts[['AlertName', 'TimeGenerated']]\\\n                        .groupby('AlertName').TimeGenerated.agg('count').to_dict())\n    # acct_alert_items = related_alerts\\\n    #     .query('acct_match == @True')[['AlertType', 'StartTimeUtc']]\\\n    #     .groupby('AlertType').StartTimeUtc.agg('count').to_dict()\n    # proc_alert_items = related_alerts\\\n    #     .query('proc_match == @True')[['AlertType', 'StartTimeUtc']]\\\n    #     .groupby('AlertType').StartTimeUtc.agg('count').to_dict()\n\n    def print_related_alerts(alertDict, entityType, entityName):\n        if len(alertDict) > 0:\n            display(Markdown('### Found {} different alert types related to this {} (\\'{}\\')'.format(len(alertDict), entityType, entityName)))\n            for (k,v) in alertDict.items():\n                print('- {}, Count of alerts: {}'.format(k, v))\n        else:\n            print('No alerts for {} entity \\'{}\\''.format(entityType, entityName))\n\n    print_related_alerts(host_alert_items, 'host', host_entity.HostName)\n\n    nbdisp.display_timeline(data=related_alerts, title=\"Alerts\", source_columns=['AlertName'], height=200)\nelse:\n    display(Markdown('No related alerts found.'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Browse List of Related Alerts\nSelect an Alert to view details"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def disp_full_alert(alert):\n    global related_alert\n    related_alert = mas.SecurityAlert(alert)\n    nbdisp.display_alert(related_alert, show_entities=True)\n\nif related_alerts is not None and not related_alerts.empty:\n    related_alerts['CompromisedEntity'] = related_alerts['Computer']\n    display(Markdown('### Click on alert to view details.'))\n    rel_alert_select = mas.AlertSelector(alerts=related_alerts, \n    #                                      columns=['TimeGenerated', 'AlertName', 'CompromisedEntity', 'SystemAlertId'],\n                                         action=disp_full_alert)\n    rel_alert_select.display()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='host_logons'></a>[Contents](#toc)\n# Host Logons"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from msticpy.nbtools.query_defns import DataFamily, DataEnvironment\nparams_dict = {}\nparams_dict['host_filter_eq'] = f'Computer has \\'{host_entity.HostName}\\''\nparams_dict['host_filter_neq'] = f'Computer !has \\'{host_entity.HostName}\\''\nparams_dict['host_name'] = host_entity.HostName\nparams_dict['subscription_filter'] = 'true'\nif host_entity.OSFamily == 'Linux':\n    params_dict['data_family'] = DataFamily.LinuxSecurity\n    params_dict['path_separator'] = '/'\nelse:\n    params_dict['data_family'] = DataFamily.WindowsSecurity\n    params_dict['path_separator'] = '\\\\'\n\n# set the origin time to the time of our alert\nlogon_query_times = mas.QueryTime(units='day',\n                                  before=5, after=1, max_before=20, max_after=20)\nlogon_query_times.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nfrom msticpy.sectools.eventcluster import dbcluster_events, add_process_features, _string_score\n\nhost_logons = qry.list_host_logons(provs=[logon_query_times], **params_dict)\n\n%matplotlib inline\n\nif host_logons is not None and not host_logons.empty:\n    logon_features = host_logons.copy()\n    logon_features['AccountNum'] = host_logons.apply(lambda x: _string_score(x.Account), axis=1)\n    logon_features['LogonIdNum'] = host_logons.apply(lambda x: _string_score(x.TargetLogonId), axis=1)\n    logon_features['LogonHour'] = host_logons.apply(lambda x: x.TimeGenerated.hour, axis=1)\n\n    # you might need to play around with the max_cluster_distance parameter.\n    # decreasing this gives more clusters.\n    (clus_logons, _, _) = dbcluster_events(data=logon_features, time_column='TimeGenerated',\n                                           cluster_columns=['AccountNum',\n                                                            'LogonType'],\n                                           max_cluster_distance=0.0001)\n    display(Markdown(f'Number of input events: {len(host_logons)}'))\n    display(Markdown(f'Number of clustered events: {len(clus_logons)}'))\n    display(Markdown('### Distinct host logon patterns'))\n    clus_logons.sort_values('TimeGenerated')\n    nbdisp.display_logon_data(clus_logons)\n    \n    display(Markdown('### Logon timeline.'))\n    tooltip_cols = ['TargetUserName', 'TargetDomainName', 'SubjectUserName', \n                    'SubjectDomainName', 'LogonType', 'IpAddress']\n    nbdisp.display_timeline(data=host_logons.query('TargetLogonId != \"0x3e7\"'),\n                            overlay_data=host_logons.query('TargetLogonId == \"0x3e7\"'),\n                            title=\"Logons (blue=user, green=system)\", \n                            source_columns=tooltip_cols, height=200)\n    \n    display(Markdown('### Counts of logon events by logon type.'))\n    display(Markdown('Min counts for each logon type highlighted.'))\n    logon_by_type = (host_logons[['Account', 'LogonType', 'EventID']]\n                    .groupby(['Account','LogonType']).count().unstack()\n                    .fillna(0)\n                    .style\n                    .background_gradient(cmap='viridis', low=.5, high=0)\n                    .format(\"{0:0>3.0f}\"))\n    display(logon_by_type)\n    key = 'logon type key = {}'.format('; '.join([f'{k}: {v}' for k,v in mas.nbdisplay._WIN_LOGON_TYPE_MAP.items()]))\n    display(Markdown(key))\n    \n    display(Markdown('### Relative frequencies by account'))\n    plt.rcParams['figure.figsize'] = (12, 4)\n    clus_logons.plot.barh(x=\"Account\", y=\"ClusterSize\")\nelse:\n    display(Markdown('No logon events found for host.'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true
      },
      "cell_type": "markdown",
      "source": "<a id='failed logons'></a>[Contents](#toc)\n### Failed Logons"
    },
    {
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "failedLogons = qry.list_host_logon_failures(provs=[query_times], **params_dict)\nif failedLogons.shape[0] == 0:\n    display(print('No logon failures recorded for this host between {security_alert.start} and {security_alert.start}'))\n\nfailedLogons",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='examine_win_logon_sess'></a>[Contents](#toc)\n## Examine a Logon Session\n\n### Select a Logon ID To Examine"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\ndist_logons = clus_logons.sort_values('TimeGenerated')[['TargetUserName', 'TimeGenerated', \n                                                        'LastEventTime', 'LogonType', \n                                                        'ClusterSize']]\nitems = dist_logons.apply(lambda x: (f'{x.TargetUserName}:    '\n                                     f'(logontype={x.LogonType})   '\n                                     f'timerange={x.TimeGenerated} - {x.LastEventTime}    '\n                                     f'count={x.ClusterSize}'),\n                          axis=1).values.tolist()\n\ndef get_selected_logon_cluster(selected_item):\n    acct_match = re.search(r'(?P<acct>[^:]+):\\s+\\(logontype=(?P<l_type>[^)]+)', selected_item)\n    if acct_match:\n        acct = acct_match['acct']\n        l_type = int(acct_match['l_type'])\n        return host_logons.query('TargetUserName == @acct and LogonType == @l_type')\n\nlogon_list_regex = r'''\n(?P<acct>[^:]+):\\s+\n\\(logontype=(?P<logon_type>[^)]+)\\)\\s+\n\\(timestamp=(?P<time>[^)]+)\\)\\s+\nlogonid=(?P<logonid>[0-9a-fx)]+)\n'''\n\ndef get_selected_logon(selected_item):\n    acct_match = re.search(logon_list_regex, selected_item, re.VERBOSE)\n    if acct_match:\n        acct = acct_match['acct']\n        logon_type = int(acct_match['logon_type'])\n        time_stamp = pd.to_datetime(acct_match['time'])\n        logon_id = acct_match['logonid']\n        return host_logons.query('TargetUserName == @acct and LogonType == @logon_type'\n                                 ' and TargetLogonId == @logon_id')\n    \nlogon_wgt = mas.SelectString(description='Select logon cluster to examine', \n                             item_list=items, height='200px', width='100%', auto_display=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Calculate time range based on the logons from previous section\nselected_logon_cluster = get_selected_logon_cluster(logon_wgt.value)\n\nif len(selected_logon_cluster) > 20:\n    display(Markdown('<h3><p style=\"color:red\">Warning: the selected '\n                     'cluster has a high number of logons.</p></h1><br>'\n                     'Processes for these logons may be very slow '\n                     'to retrieve and result in high memory usage.<br>'\n                     'You may wish to narrow the time range and sample'\n                     'the data before running the query for the full range.'))\n    \nlogon_time = selected_logon_cluster['TimeGenerated'].min()\nlast_logon_time = selected_logon_cluster['TimeGenerated'].max()\ntime_diff = int((last_logon_time - logon_time).total_seconds() / (60 * 60) + 2)\n\n# set the origin time to the time of our alert\nproc_query_times = mas.QueryTime(units='hours', origin_time=logon_time,\n                           before=1, after=time_diff, max_before=60, max_after=120)\nproc_query_times.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from msticpy.sectools.eventcluster import dbcluster_events, add_process_features\nprint('Getting process events...', end='')\nprocesses_on_host = qry.list_processes(provs=[proc_query_times], **params_dict)\nprint('done')\nprint('Clustering...', end='')\nfeature_procs = add_process_features(input_frame=processes_on_host,\n                                     path_separator=params_dict['path_separator'])\n\nfeature_procs['accountNum'] = feature_procs.apply(lambda x: _string_score(x.Account), axis=1)\n# you might need to play around with the max_cluster_distance parameter.\n# decreasing this gives more clusters.\n(clus_events, dbcluster, x_data) = dbcluster_events(data=feature_procs,\n                                                    cluster_columns=['commandlineTokensFull', \n                                                                     'pathScore',\n                                                                     'accountNum',\n                                                                     'isSystemSession'],\n                                                    max_cluster_distance=0.0001)\nprint('done')\nprint('Number of input events:', len(feature_procs))\nprint('Number of clustered events:', len(clus_events))\n\ndef view_logon_sess(x=''):\n    global selected_logon\n    selected_logon = get_selected_logon(x)\n    logonId = selected_logon['TargetLogonId'].iloc[0]\n    sess_procs = (processes_on_host.query('TargetLogonId == @logonId | SubjectLogonId == @logonId')\n                                          [['NewProcessName', 'CommandLine', 'TargetLogonId']]\n                  .drop_duplicates())\n    display(sess_procs)\n\nselected_logon_cluster = get_selected_logon_cluster(logon_wgt.value)\n    \nselected_tgt_logon = selected_logon_cluster['TargetUserName'].iat[0]\nsystem_logon = selected_tgt_logon.lower() == 'system' or selected_tgt_logon.endswith('$')\n\nif system_logon:\n    \n    display(Markdown('<h3><p style=\"color:red\">Warning: the selected '\n                     'account name appears to be a system account.</p></h1><br>'\n                     '<i>It is difficult to accurately associate processes '\n                     'with the specific logon sessions.<br>'\n                     'Showing clustered events for entire time selection.'))\n    display(clus_events.sort_values('TimeGenerated')[['TimeGenerated', 'LastEventTime',\n                                    'NewProcessName', 'CommandLine', \n                                    'ClusterSize', 'commandlineTokensFull',\n                                    'pathScore', 'isSystemSession']])\n\n# Display a pick list for logon instances\nitems = (host_logons.query('TargetUserName == @selected_tgt_logon')\n         .sort_values('TimeGenerated')\n         .apply(lambda x: (f'{x.TargetUserName}:    '\n                           f'(logontype={x.LogonType})   '\n                           f'(timestamp={x.TimeGenerated})    '\n                           f'logonid={x.TargetLogonId}'),\n                axis=1).values.tolist())\nsess_w = widgets.Select(options=items, description='Select logon instance to examine', **WIDGET_DEFAULTS)\n\nwidgets.interactive(view_logon_sess, x=sess_w)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='cmdlineiocs'></a>[Contents](#toc)\n# Check for IOCs in Commandline for current session\nThis section looks for Indicators of Compromise (IoC) within the data sets passed to it.\n\nThe first section looks at the commandlines for the processes in the selected session. It also looks for base64 encoded strings within the data - this is a common way of hiding attacker intent. It attempts to decode any strings that look like base64. Additionally, if the base64 decode operation returns any items that look like a base64 encoded string or file, a gzipped binary sequence, a zipped or tar archive, it will attempt to extract the contents before searching for potentially interesting IoC observables within the decoded data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if not system_logon:\n    logonId = selected_logon['TargetLogonId'].iloc[0]\n    sess_procs = (processes_on_host.query('TargetLogonId == @logonId | SubjectLogonId == @logonId'))\nelse:\n    sess_procs = clus_events\n    \nioc_extractor = sectools.IoCExtract()\nos_family = host_entity.OSType if host_entity.OSType else 'Windows'\n\nioc_df = ioc_extractor.extract(data=sess_procs, \n                               columns=['CommandLine'],\n                               os_family=os_family,\n                               ioc_types=['ipv4', 'ipv6', 'dns', 'url',\n                                          'md5_hash', 'sha1_hash', 'sha256_hash'])\nif len(ioc_df):\n    display(Markdown(\"### IoC patterns found in process set.\"))\n    display(ioc_df)\nelse:\n    display(Markdown(\"### No IoC patterns found in process tree.\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### If any Base64 encoded strings, decode and search for IoCs in the results.\nFor simple strings the Base64 decoded output is straightforward. However for nested encodings this can get a little complex and difficult to represent in a tabular format.\n\n**Columns**\n - reference - The index of the row item in dotted notation in depth.seq pairs (e.g. 1.2.2.3 would be the 3 item at depth 3 that is a child of the 2nd item found at depth 1). This may not always be an accurate notation - it is mainly use to allow you to associate an individual row with the reference value contained in the full_decoded_string column of the topmost item).\n - original_string - the original string before decoding.\n - file_name - filename, if any (only if this is an item in zip or tar file).\n - file_type - a guess at the file type (this is currently elementary and only includes a few file types).\n - input_bytes - the decoded bytes as a Python bytes string.\n - decoded_string - the decoded string if it can be decoded as a UTF-8 or UTF-16 string. Note: binary sequences may often successfully decode as UTF-16 strings but, in these cases, the decodings are meaningless.\n - encoding_type - encoding type (UTF-8 or UTF-16) if a decoding was possible, otherwise 'binary'.\n - file_hashes - collection of file hashes for any decoded item.\n - md5 - md5 hash as a separate column.\n - sha1 - sha1 hash as a separate column.\n - sha256 - sha256 hash as a separate column.\n - printable_bytes - printable version of input_bytes as a string of \\xNN values\n - src_index - the index of the row in the input dataframe from which the data came.\n - full_decoded_string - the full decoded string with any decoded replacements. This is only really useful for top-level items, since nested items will only show the 'full' string representing the child fragment."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "dec_df = sectools.b64.unpack_items(data=sess_procs, column='CommandLine')\nif len(dec_df) > 0:\n    display(HTML(\"<h3>Decoded base 64 command lines</h3>\"))\n    display(HTML(\"Decoded values and hashes of decoded values shown below.\"))\n    display(HTML('Warning - some binary patterns may be decodable as unicode strings. '\n                 'In these cases you should ignore the \"decoded_string\" column '\n                 'and treat the encoded item as a binary - using the \"printable_bytes\" '\n                 'column or treat the decoded_string as a binary (bytes) value.'))\n    \n    display(dec_df[['full_decoded_string', 'decoded_string', 'original_string', 'printable_bytes', 'file_hashes']])\n\n    ioc_dec_df = ioc_extractor.extract(data=dec_df, columns=['full_decoded_string'])\n    if len(ioc_dec_df):\n        display(HTML(\"<h3>IoC patterns found in events with base64 decoded data</h3>\"))\n        display(ioc_dec_df)\n        ioc_df = ioc_df.append(ioc_dec_df ,ignore_index=True)\nelse:\n    print(\"No base64 encodings found.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true
      },
      "cell_type": "markdown",
      "source": "<a id='virustotallookup'></a>[Contents](#toc)\n## Virus Total Lookup\nThis section uses the popular Virus Total service to check any recovered IoCs against VTs database.\n\nTo use this you need an API key from virus total, which you can obtain here: https://www.virustotal.com/.\n\nNote that VT throttles requests for free API keys to 4/minute. If you are unable to process the entire data set, try splitting it and submitting smaller chunks.\n\n**Things to note:**\n- Virus Total lookups include file hashes, domains, IP addresses and URLs.\n- The returned data is slightly different depending on the input type\n- The VTLookup class tries to screen input data to prevent pointless lookups. E.g.:\n  - Only public IP Addresses will be submitted (no loopback, private address space, etc.)\n  - URLs with only local (unqualified) host parts will not be submitted.\n  - Domain names that are unqualified will not be submitted.\n  - Hash-like strings (e.g 'AAAAAAAAAAAAAAAAAA') that do not appear to have enough entropy to be a hash will not be submitted.\n\n**Output Columns**\n - Observable - The IoC observable submitted\n - IoCType - the IoC type\n - Status - the status of the submission request\n - ResponseCode - the VT response code\n - RawResponse - the entire raw json response\n - Resource - VT Resource\n - SourceIndex - The index of the Observable in the source DataFrame. You can use this to rejoin to your original data.\n - VerboseMsg - VT Verbose Message\n - ScanId - VT Scan ID if any\n - Permalink - VT Permanent URL describing the resource\n - Positives - If this is not zero, it indicates the number of malicious reports that VT holds for this observable.\n - MD5 - The MD5 hash, if any\n - SHA1 - The MD5 hash, if any\n - SHA256 - The MD5 hash, if any\n - ResolvedDomains - In the case of IP Addresses, this contains a list of all domains that resolve to this IP address\n - ResolvedIPs - In the case Domains, this contains a list of all IP addresses resolved from the domain.\n - DetectedUrls - Any malicious URLs associated with the observable."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vt_key = mas.GetEnvironmentKey(env_var='VT_API_KEY',\n                           help_str='To obtain an API key sign up here https://www.virustotal.com/',\n                           prompt='Virus Total API key:')\nvt_key.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "if vt_key.value and ioc_df is not None and not ioc_df.empty:\n    vt_lookup = sectools.VTLookup(vt_key.value, verbosity=2)\n\n    print(f'{len(ioc_df)} items in input frame')\n    supported_counts = {}\n    for ioc_type in vt_lookup.supported_ioc_types:\n        supported_counts[ioc_type] = len(ioc_df[ioc_df['IoCType'] == ioc_type])\n    print('Items in each category to be submitted to VirusTotal')\n    print('(Note: items have pre-filtering to remove obvious erroneous '\n          'data and false positives, such as private IPaddresses)')\n    print(supported_counts)\n    print('-' * 80)\n    vt_results = vt_lookup.lookup_iocs(data=ioc_df, type_col='IoCType', src_col='Observable')\n    display(vt_results)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='comms_to_other_hosts'></a>[Contents](#toc)\n# Network Check Communications with Other Hosts"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Azure Network Analytics Base Query\n\n    \naz_net_analytics_query =r'''\nAzureNetworkAnalytics_CL \n| where SubType_s == 'FlowLog'\n| where FlowStartTime_t >= datetime({start})\n| where FlowEndTime_t <= datetime({end})\n| project TenantId, TimeGenerated, \n    FlowStartTime = FlowStartTime_t, \n    FlowEndTime = FlowEndTime_t, \n    FlowIntervalEndTime = FlowIntervalEndTime_t, \n    FlowType = FlowType_s,\n    ResourceGroup = split(VM_s, '/')[0],\n    VMName = split(VM_s, '/')[1],\n    VMIPAddress = VMIP_s, \n    PublicIPs = extractall(@\"([\\d\\.]+)[|\\d]+\", dynamic([1]), PublicIPs_s),\n    SrcIP = SrcIP_s,\n    DestIP = DestIP_s,\n    ExtIP = iif(FlowDirection_s == 'I', SrcIP_s, DestIP_s),\n    L4Protocol = L4Protocol_s, \n    L7Protocol = L7Protocol_s, \n    DestPort = DestPort_d, \n    FlowDirection = FlowDirection_s,\n    AllowedOutFlows = AllowedOutFlows_d, \n    AllowedInFlows = AllowedInFlows_d,\n    DeniedInFlows = DeniedInFlows_d, \n    DeniedOutFlows = DeniedOutFlows_d,\n    RemoteRegion = AzureRegion_s,\n    VMRegion = Region_s\n| extend AllExtIPs = iif(isempty(PublicIPs), pack_array(ExtIP), \n                         iif(isempty(ExtIP), PublicIPs, array_concat(PublicIPs, pack_array(ExtIP)))\n                         )\n| project-away ExtIP\n| mvexpand AllExtIPs\n{where_clause}\n'''\n\nip_q_times = mas.QueryTime(label='Set time bounds for network queries',\n                           units='hour', max_before=48, before=10, after=5, \n                           max_after=24)\nip_q_times.display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Query Flows by Host IP Addresses"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if 'AzureNetworkAnalytics_CL' not in table_index:\n    print('No network flow data available.')\n    az_net_comms_df = None\nelse:\n    all_host_ips = host_entity.private_ips + host_entity.public_ips + [host_entity.IPAddress]\n    host_ips = {'\\'{}\\''.format(i.Address) for i in all_host_ips}\n    host_ip_list = ','.join(host_ips)\n\n    if not host_ip_list:\n        raise ValueError('No IP Addresses for host. Cannot lookup network data')\n\n    az_ip_where = f'''\n    | where (VMIPAddress in ({host_ip_list}) \n            or SrcIP in ({host_ip_list}) \n            or DestIP in ({host_ip_list}) \n            ) and \n        (AllowedOutFlows > 0 or AllowedInFlows > 0)'''\n    print('getting data...')\n    az_net_query_byip = az_net_analytics_query.format(where_clause=az_ip_where,\n                                                      start = ip_q_times.start,\n                                                      end = ip_q_times.end)\n\n    net_default_cols = ['FlowStartTime', 'FlowEndTime', 'VMName', 'VMIPAddress', \n                    'PublicIPs', 'SrcIP', 'DestIP', 'L4Protocol', 'L7Protocol',\n                    'DestPort', 'FlowDirection', 'AllowedOutFlows', \n                    'AllowedInFlows']\n\n    %kql -query az_net_query_byip\n    az_net_comms_df = _kql_raw_result_.to_dataframe()\n    az_net_comms_df[net_default_cols]\n\n    if len(az_net_comms_df) > 0:\n        import warnings\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n            az_net_comms_df['TotalAllowedFlows'] = az_net_comms_df['AllowedOutFlows'] + az_net_comms_df['AllowedInFlows']\n            sns.catplot(x=\"L7Protocol\", y=\"TotalAllowedFlows\", col=\"FlowDirection\", data=az_net_comms_df)\n            sns.relplot(x=\"FlowStartTime\", y=\"TotalAllowedFlows\", \n                        col=\"FlowDirection\", kind=\"line\", \n                        hue=\"L7Protocol\", data=az_net_comms_df).set_xticklabels(rotation=50)\n\n        nbdisp.display_timeline(data=az_net_comms_df.query('AllowedOutFlows > 0'),\n                                 overlay_data=az_net_comms_df.query('AllowedInFlows > 0'),\n                                 title='Network Flows (out=blue, in=green)',\n                                 time_column='FlowStartTime',\n                                 source_columns=['FlowType', 'AllExtIPs', 'L7Protocol', 'FlowDirection'],\n                                 height=300)\n    else:\n        print('No network data for specified time range.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Flow Summary"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if az_net_comms_df is not None and not az_net_comms_df.empty:\n    cm = sns.light_palette(\"green\", as_cmap=True)\n\n    cols = ['VMName', 'VMIPAddress', 'PublicIPs', 'SrcIP', 'DestIP', 'L4Protocol',\n            'L7Protocol', 'DestPort', 'FlowDirection', 'AllExtIPs', 'TotalAllowedFlows']\n    flow_index = az_net_comms_df[cols].copy()\n    def get_source_ip(row):\n        if row.FlowDirection == 'O':\n            return row.VMIPAddress if row.VMIPAddress else row.SrcIP\n        else:\n            return row.AllExtIPs if row.AllExtIPs else row.DestIP\n\n    def get_dest_ip(row):\n        if row.FlowDirection == 'O':\n            return row.AllExtIPs if row.AllExtIPs else row.DestIP\n        else:\n            return row.VMIPAddress if row.VMIPAddress else row.SrcIP\n\n    flow_index['source'] = flow_index.apply(get_source_ip, axis=1)\n    flow_index['target'] = flow_index.apply(get_dest_ip, axis=1)\n    flow_index['value'] = flow_index['L7Protocol']\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        display(flow_index[['source', 'target', 'value', 'L7Protocol', \n                            'FlowDirection', 'TotalAllowedFlows']]\n         .groupby(['source', 'target', 'value', 'L7Protocol', \n                   'FlowDirection'])\n         .sum().unstack().style.background_gradient(cmap=cm))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## GeoIP Map of External IPs"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from msticpy.nbtools.foliummap import FoliumMap\nfolium_map = FoliumMap()\n\nif az_net_comms_df is None or az_net_comms_df.empty:\n    print('No network flow data available.')\nelse:    \n    ip_locs_in = set()\n    ip_locs_out = set()\n    for _, row in az_net_comms_df.iterrows():\n        ip = row.AllExtIPs\n\n        if ip in ip_locs_in or ip in ip_locs_out or not ip:\n            continue\n        ip_entity = mas.IpAddress(Address=ip)\n        iplocation.lookup_ip(ip_entity=ip_entity)\n        if not ip_entity.Location:\n            continue\n        ip_entity.AdditionalData['protocol'] = row.L7Protocol\n        if row.FlowDirection == 'I':\n            ip_locs_in.add(ip_entity)\n        else:\n            ip_locs_out.add(ip_entity)\n\n    display(HTML('<h3>External IP Addresses communicating with host</h3>'))\n    display(HTML('Numbered circles indicate multiple items - click to expand'))\n    display(HTML('Location markers: Blue = outbound, Purple = inbound, Green = Host'))\n\n    icon_props = {'color': 'green'}\n    folium_map.add_ip_cluster(ip_entities=host_entity.public_ips,\n                                **icon_props)\n    icon_props = {'color': 'blue'}\n    folium_map.add_ip_cluster(ip_entities=ip_locs_out,\n                                **icon_props)\n    icon_props = {'color': 'purple'}\n    folium_map.add_ip_cluster(ip_entities=ip_locs_in,\n                                **icon_props)\n\n    display(folium_map.folium_map)\n    display(Markdown('<p style=\"color:red\">Warning: the folium mapping library '\n                     'does not display correctly in some browsers.</p><br>'\n                     'If you see a blank image please retry with a different browser.'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true
      },
      "cell_type": "markdown",
      "source": "<a id='appendices'></a>[Contents](#toc)\n# Appendices"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Available DataFrames"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('List of current DataFrames in Notebook')\nprint('-' * 50)\ncurrent_vars = list(locals().keys())\nfor var_name in current_vars:\n    if isinstance(locals()[var_name], pd.DataFrame) and not var_name.startswith('_'):\n        print(var_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "tags": [
          "todo"
        ]
      },
      "cell_type": "markdown",
      "source": "## Saving Data to Excel\nTo save the contents of a pandas DataFrame to an Excel spreadsheet\nuse the following syntax\n```\nwriter = pd.ExcelWriter('myWorksheet.xlsx')\nmy_data_frame.to_excel(writer,'Sheet1')\nwriter.save()\n```"
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "318.996px",
        "width": "320.994px"
      },
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents2",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "406.193px",
        "left": "1468.4px",
        "right": "20px",
        "top": "120px",
        "width": "456.572px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}